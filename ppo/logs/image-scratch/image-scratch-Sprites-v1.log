[INFO] Learning Configuration:
            Policy
                - Policy type:      image-scratch
                - Policy network:   MLPActorCritic(
  (actor): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=2, bias=True)
    (3): Tanh()
  )
  (critic): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=1, bias=True)
  )
  (encoder): ImageEncoder(
    (convs): Sequential(
      (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): ReLU()
      (2): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (3): ReLU()
      (4): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (5): ReLU()
      (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (7): ReLU()
      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (9): ReLU()
      (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (projection): Linear(in_features=128, out_features=64, bias=True)
  )
)
            Environment
                - Env name:         Sprites-v1
                - Num envs:         4
                - Spec:             EnvSpec(id='Sprites-v1', entry_point='sprites_env.envs.sprites:SpritesEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, disable_env_checker=False, kwargs={'n_distractors': 1, 'num_envs': 4, 'vectorization_mode': 'sync'}, namespace=None, name='Sprites', version=1, additional_wrappers=(), vector_entry_point=None)
            Rollout
                - Rollout Steps:    2048
            Training
                - Total timesteps:  4194304
                - N epochs:         10
                - Batch size:       64
                - gamma (discount): 0.99
                - GAE lambda:       0.95
                - clip range:       0.2
                - Normalize adv:    True
                - Entropy coef:     0.01
                - Value coef:       0.5
                - Optimizer:        Adam
                - Learning rate:    0.0003
            Device
                - Device:           cuda
            Verbosity
                - Verbose level:    1
                - Log interval:     10
                - Tensorboard log:  ppo/TB_PPO
        
[INFO] Update 0 (0 / 4194304 steps)
    (Validation) Returns: 28.668432017775217 +/- 2.858689671577409
[INFO] Update 1 (8192 / 4194304 steps)
