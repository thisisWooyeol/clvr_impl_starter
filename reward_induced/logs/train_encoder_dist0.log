[INFO] Training Reward Predictor on device: cuda
[INFO] Training with the following configuration:
        - Input shape: (3,64,64)
        - Number of frames: 1
        - Number of future frames: 29
        - Reward types: ['agent_x', 'agent_y', 'target_x', 'target_y']
        - Number of iterations: 1500
        - Shapes per trajectory: 2
        - Batch size: 64
[100/1500] Loss: 0.05324195325374603
[200/1500] Loss: 0.05209924280643463
[300/1500] Loss: 0.050121817737817764
[400/1500] Loss: 0.03622281178832054
Intermediate model save at reward_induced/models/encoder/encoder_dist0_400.pt
[500/1500] Loss: 0.02598263882100582
[600/1500] Loss: 0.018971795216202736
[700/1500] Loss: 0.011190636083483696
[800/1500] Loss: 0.0062310947105288506
Intermediate model save at reward_induced/models/encoder/encoder_dist0_800.pt
[900/1500] Loss: 0.006202739663422108
[1000/1500] Loss: 0.005459592677652836
[1100/1500] Loss: 0.004607384093105793
[1200/1500] Loss: 0.003968843258917332
Intermediate model save at reward_induced/models/encoder/encoder_dist0_1200.pt
[1300/1500] Loss: 0.004114944953471422
[1400/1500] Loss: 0.003957575652748346
[1500/1500] Loss: 0.0038524740375578403
Loss plot saved at reward_induced/logs/encoder_dist0.png
Final model saved at reward_induced/models/encoder/encoder_dist0_final.pt
Training complete.
