[INFO] Training Reward Predictor on device: cuda
[INFO] Training with the following configuration:
        - Input shape: (3,64,64)
        - Number of frames: 1
        - Number of future frames: 29
        - Reward types: ['vertical_position']
        - Number of iterations: 1000
        - Shapes per trajectory: 1
        - Batch size: 64
[100/1000] Loss: 0.049675531685352325
[200/1000] Loss: 0.04929466173052788
[300/1000] Loss: 0.04788674786686897
[400/1000] Loss: 0.0026151305064558983
Intermediate model save at reward_induced/models/encoder/encoder_VertPosReward_400.pt
[500/1000] Loss: 0.0010693197837099433
[600/1000] Loss: 0.0006101974286139011
[700/1000] Loss: 0.0004924886743538082
[800/1000] Loss: 0.0006159906624816358
Intermediate model save at reward_induced/models/encoder/encoder_VertPosReward_800.pt
[900/1000] Loss: 0.00029988805181346834
[1000/1000] Loss: 0.00024691945873200893
Loss plot saved at reward_induced/logs/encoder_VertPosReward.png
Final model saved at reward_induced/models/encoder/encoder_VertPosReward_final.pt
Training complete.
