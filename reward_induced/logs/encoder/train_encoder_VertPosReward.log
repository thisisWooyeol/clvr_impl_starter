[INFO] Training Reward Predictor on device: cuda
[INFO] Training with the following configuration:
        - Input shape: (3,64,64)
        - Number of frames: 1
        - Number of future frames: 29
        - Reward types: ['vertical_position']
        - Number of iterations: 1000
        - Shapes per trajectory: 1
        - Batch size: 64
[100/1000] Loss: 0.06027821823954582
[200/1000] Loss: 0.05056595802307129
[300/1000] Loss: 0.0062516434118151665
[400/1000] Loss: 0.0006530562532134354
Intermediate model save at reward_induced/models/encoder/encoder_VertPosReward_400.pt
[500/1000] Loss: 0.00031092483550310135
[600/1000] Loss: 0.00021792018378619105
[700/1000] Loss: 0.00019898920436389744
[800/1000] Loss: 0.0001652887585805729
Intermediate model save at reward_induced/models/encoder/encoder_VertPosReward_800.pt
[900/1000] Loss: 0.0001477628102293238
[1000/1000] Loss: 0.00014303305943030864
Loss plot saved at reward_induced/logs/encoder/encoder_VertPosReward.png
Final model saved at reward_induced/models/encoder/encoder_VertPosReward_final.pt
Training complete.
