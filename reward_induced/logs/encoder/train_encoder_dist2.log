[INFO] Training Reward Predictor on device: cuda
[INFO] Training with the following configuration:
        - Input shape: (3,64,64)
        - Number of frames: 1
        - Number of future frames: 29
        - Reward types: ['agent_x', 'agent_y', 'target_x', 'target_y']
        - Number of iterations: 1500
        - Shapes per trajectory: 4
        - Batch size: 64
[100/1500] Loss: 0.05625094100832939
[200/1500] Loss: 0.05784907937049866
[300/1500] Loss: 0.04534014314413071
[400/1500] Loss: 0.030148399993777275
Intermediate model save at reward_induced/models/encoder/encoder_dist2_400.pt
[500/1500] Loss: 0.026483863592147827
[600/1500] Loss: 0.02364804781973362
[700/1500] Loss: 0.02081102691590786
[800/1500] Loss: 0.021405672654509544
Intermediate model save at reward_induced/models/encoder/encoder_dist2_800.pt
[900/1500] Loss: 0.01056633610278368
[1000/1500] Loss: 0.0074648018926382065
[1100/1500] Loss: 0.0077144247479736805
[1200/1500] Loss: 0.0057239537127316
Intermediate model save at reward_induced/models/encoder/encoder_dist2_1200.pt
[1300/1500] Loss: 0.004742113873362541
[1400/1500] Loss: 0.0043486920185387135
[1500/1500] Loss: 0.003723033471032977
Loss plot saved at reward_induced/logs/encoder_dist2.png
Final model saved at reward_induced/models/encoder/encoder_dist2_final.pt
Training complete.
