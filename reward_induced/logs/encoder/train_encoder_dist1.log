[INFO] Training Reward Predictor on device: cuda
[INFO] Training with the following configuration:
        - Input shape: (3,64,64)
        - Number of frames: 1
        - Number of future frames: 29
        - Reward types: ['agent_x', 'agent_y', 'target_x', 'target_y']
        - Number of iterations: 1500
        - Shapes per trajectory: 3
        - Batch size: 64
[100/1500] Loss: 0.05420350283384323
[200/1500] Loss: 0.052103716880083084
[300/1500] Loss: 0.05203729122877121
[400/1500] Loss: 0.03364969789981842
Intermediate model save at reward_induced/models/encoder/encoder_dist1_400.pt
[500/1500] Loss: 0.02880292944610119
[600/1500] Loss: 0.02579430304467678
[700/1500] Loss: 0.02262791432440281
[800/1500] Loss: 0.017432907596230507
Intermediate model save at reward_induced/models/encoder/encoder_dist1_800.pt
[900/1500] Loss: 0.016180066391825676
[1000/1500] Loss: 0.017019622027873993
[1100/1500] Loss: 0.017286043614149094
[1200/1500] Loss: 0.008909651078283787
Intermediate model save at reward_induced/models/encoder/encoder_dist1_1200.pt
[1300/1500] Loss: 0.005450695753097534
[1400/1500] Loss: 0.00432221032679081
[1500/1500] Loss: 0.004067199304699898
Loss plot saved at reward_induced/logs/encoder/encoder_dist1.png
Final model saved at reward_induced/models/encoder/encoder_dist1_final.pt
Training complete.
