[INFO] Training Reward Predictor on device: cuda
[INFO] Training with the following configuration:
        - Input shape: (3,64,64)
        - Number of frames: 1
        - Number of future frames: 29
        - Reward types: ['agent_x', 'agent_y', 'target_x', 'target_y']
        - Number of iterations: 1500
        - Shapes per trajectory: 2
        - Batch size: 64
[100/1500] Loss: 0.059888459742069244
[200/1500] Loss: 0.05339158698916435
[300/1500] Loss: 0.029431700706481934
[400/1500] Loss: 0.01307447999715805
Intermediate model save at reward_induced/models/encoder/encoder_dist0_400.pt
[500/1500] Loss: 0.006755803246051073
[600/1500] Loss: 0.0052605364471673965
[700/1500] Loss: 0.004593617748469114
[800/1500] Loss: 0.003627703059464693
Intermediate model save at reward_induced/models/encoder/encoder_dist0_800.pt
[900/1500] Loss: 0.002901441417634487
[1000/1500] Loss: 0.002541858470067382
[1100/1500] Loss: 0.0025456261355429888
[1200/1500] Loss: 0.0020721254404634237
Intermediate model save at reward_induced/models/encoder/encoder_dist0_1200.pt
[1300/1500] Loss: 0.0018166007939726114
[1400/1500] Loss: 0.0017537972889840603
[1500/1500] Loss: 0.0014438556972891092
Loss plot saved at reward_induced/logs/encoder/encoder_dist0.png
Final model saved at reward_induced/models/encoder/encoder_dist0_final.pt
Training complete.
